{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3가지 감정분류 classification.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"eM2gbjEvPRue"},"source":["# 구글 드라이브와 연동합니다\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8fUJRnIMPdmJ"},"source":["from google.colab import files\n","myfile = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kx2cDV4QRN5R"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"liZ_MmyxQXes"},"source":["import io\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjTWtw3dQ2y3"},"source":["train_data = pd.read_csv(io.BytesIO(myfile['final_dataset.csv']),encoding = \"CP949\")\n","train_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-zwSjkxUMpx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5cXWQNYPXsB"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat Nov 21 14:02:51 2020\n","\n","@author: 현\n","\"\"\"\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import konlpy\n","from konlpy.tag import Okt \n","# from tensorflow.keras.preprocessing.text import Tokenizer \n","from keras.preprocessing.text import Tokenizer\n","import numpy as np\n","\n","\n","\n","#수정_합본데이터 : 단발성+연속성 데이터 불러오기\n","# train_data = pd.read_csv(\"drive/My Drive/Colab Notebooks/korean_data_2.csvkorean_data_2.csv\")\n","#네이버리뷰 추가\n","# train_data2 = pd.read_csv(\"sample.csv\")\n","# train_data2 = train_data2[:100000]\n","# train_data = pd.concat([train_data,train_data2])\n","print(train_data.groupby('Emotion').size().reset_index(name='count'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W36XV1gOV9Vx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"stSfS9jCVmEJ"},"source":["indexNames = train_data[ train_data['Emotion'] == '놀람' ].index\n","# Delete these row indexes from dataFrame\n","train_data.drop(indexNames , inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nd4Y6rJqWJlm"},"source":["print(train_data.groupby('Emotion').size().reset_index(name='count'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQEkwp8aUPP5"},"source":["train_data.loc[(train_data['Emotion']==\"기쁨\"),'Emotion'] = '1'\n","train_data.loc[(train_data['Emotion']==\"중립\"),'Emotion'] = '0'\n","train_data.loc[(train_data['Emotion']==\"공포\"),'Emotion'] = '-1'\n","# train_data.loc[(train_data['Emotion']==\"놀람\"),'Emotion'] = '-1'\n","train_data.loc[(train_data['Emotion']==\"분노\"),'Emotion'] = '-1'\n","train_data.loc[(train_data['Emotion']==\"슬픔\"),'Emotion'] = '-1'\n","train_data.loc[(train_data['Emotion']==\"혐오\"),'Emotion'] = '-1'\n","print(train_data.groupby('Emotion').size().reset_index(name='count'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A000poxoUciD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ZbQc7yKRLj_"},"source":["#%%sentence전처리\n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize \n","\n","print(1)\n","\n","okt = Okt()\n","#불용어 제거\n","stopwords = ['의', '가', '이', '은', '들', '는', '좀', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n","\n","X_train = []\n","#돌아가는지 확인용...\n","cnt=-1 \n","for sentence in train_data['Sentence']: \n","    cnt = cnt +1 \n","    if cnt%2000==0:\n","        print(cnt)\n","    temp_X = [] \n","    temp_X = okt.morphs(sentence, stem=True) # 토큰화 morphs : 형태소 추출\n","    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거 \n","    X_train.append(temp_X) \n","    \n","\n","max_words = 38000\n","tokenizer = Tokenizer(num_words = max_words) \n","tokenizer.fit_on_texts(X_train) \n","X_train = tokenizer.texts_to_sequences(X_train) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vBFuIJAkRcuN"},"source":["#%%\n","\n","\n","print(2)\n","print(\"문장의 최대 길이 : \", max(len(l) for l in X_train)) \n","print(\"문장의 평균 길이 : \", sum(map(len, X_train))/ len(X_train)) \n","\n","# 그래프\n","plt.hist([len(s) for s in X_train], bins=50) \n","plt.xlabel('length of Data') \n","plt.ylabel('number of Data') \n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-3HWdD4XjM4"},"source":["#%%\n","\n","print(3)\n","y_train = []\n","#원핫인코딩\n","for i in range(len(train_data['Emotion'])): \n","    if train_data['Emotion'].iloc[i] == '1': \n","        y_train.append([0, 0, 1]) \n","    elif train_data['Emotion'].iloc[i] == '0':\n","        y_train.append([0, 1, 0]) \n","    elif train_data['Emotion'].iloc[i] == '-1':\n","        y_train.append([1, 0, 0])\n","\n","y_train = np.array(y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAaKRJrEX2my"},"source":["train_data['Emotion'].iloc[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gM0yzg5Xmsf"},"source":["y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkYbMe2JSGgK"},"source":["#%%데이터셋 나누기\n","from sklearn.model_selection import train_test_split\n","\n","print(4)\n","x_train, x_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.1, random_state = 100)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"357nWgtFYJN-"},"source":["#%%\n","\n","from keras.layers import Embedding, Dense, LSTM, Dropout\n","from keras.models import Sequential \n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.preprocessing.sequence import pad_sequences \n","from keras.layers import BatchNormalization\n","import keras\n","\n","print(5)\n","max_len = 15 # 전체 데이터의 길이를 15로 맞춘다 \n","\n","x_train = pad_sequences(x_train, maxlen=max_len)\n","x_test = pad_sequences(x_test, maxlen = max_len)\n","\n","'''\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-FEcA1lYOHv"},"source":["#%%\n","print(6)\n","model = Sequential()\n","model.add(Embedding(max_words,128))\n","model.add(LSTM(64, return_sequences = True))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.6)) # 드롭아웃 추가. 비율은 60%\n","model.add(LSTM(32, return_sequences = False))\n","model.add(BatchNormalization()) \n","model.add(Dropout(0.2)) # 드롭아웃 추가. 비율은 20%\n","model.add(Dense(16, activation='relu'))\n","model.add(Dropout(0.1)) # 드롭아웃 추가. 비율은 20%\n","model.add(Dense(9, activation='relu')) \n","model.add(Dense(3, activation='softmax'))\n","\n","print(7)  \n","\n","mc = ModelCheckpoint('best_model2.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","\n","opt = keras.optimizers.RMSprop(lr=0.00003)\n","model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc']) \n","history = model.fit(x_train, y_train, batch_size=100, epochs=30, callbacks=[mc], validation_data=(x_test, y_test))\n","##\n","#로스, 정확도 변화 그래프\n","epochs = range(1, len(history.history['acc']) + 1)\n","plt.plot(epochs, history.history['loss'])\n","plt.plot(epochs, history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","\n","epochs = range(1, len(history.history['acc']) + 1) \n","plt.plot(epochs, history.history['acc'])  \n","plt.plot(epochs, history.history['val_acc'])\n","plt.title('model acc')\n","plt.ylabel('acc')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skZyJimc1T4j"},"source":["model.evaluate(x_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAGYqXmY1YDx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_bYYr5DaYR-P"},"source":["#%% 입력된 문장 예측값\n","\n","from tensorflow.keras.models import load_model\n","\n","loaded_model = load_model('best_model2.h5')\n","print(\"ddd\")\n","\n","\n","def sentiment_predict(new_sentence):\n","  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n","  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n","  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n","  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n","  print(pad_new)\n","  score = loaded_model.predict(pad_new) # 예측\n","  print(score)\n","  max = np.argmax(score)\n","  \n","  if max == 0:\n","    print(\"부정 : \",end=\"\")\n","  elif max == 1:\n","    print(\"중립 : \",end=\"\")\n","  else :\n","    print(\"긍정 : \",end=\"\")\n","  return max\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fx8wK-2qrpXc"},"source":["loaded_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"glXaKMYxgrCW"},"source":["sentences = \"오늘 발표하는 날이야\"\n","sentiment_predict(sentences)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UvIAYXyqSJGW"},"source":["#sentences = \"아 오늘 겁나 짜증나\"\n","sentiment_predict(sentences)"],"execution_count":null,"outputs":[]}]}